\documentclass[12pt,a4paper]{report}
\usepackage{graphicx} % Required for inserting images
\usepackage[T2A]{fontenc} % Кодировка шрифтов
\usepackage[utf8]{inputenc} % Кодировка исходного текста
\usepackage[english, russian]{babel} % Поддержка русского языка
\usepackage{csquotes}
\usepackage{textcomp}
\usepackage{fancyhdr}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}

\geometry{left=2cm,right=2cm,top=2cm,bottom=2cm}
\begin{document}

\begin{titlepage}
\begin{center}
\bfseries

{\Large Московский авиационный институт\\ (национальный исследовательский университет)}

\vspace{48pt}

{\large Факультет информационных технологий и прикладной математики}

\vspace{36pt}

{\large Кафедра вычислительной математики и~программирования}


\vspace{48pt}

Лабораторные работы по курсу \enquote{Информационный поиск}

\end{center}

\vspace{150pt}

\begin{flushright}
    \begin{tabular}{rl}
        Студент: & В.\,А. Сухов \\
        Преподаватель: & А.\,А. Кухтичев \\
        Группа: & М8О-401Б \\
        Дата: & \\
        Оценка: & \\
        Подпись: & \\
    \end{tabular}
\end{flushright}

\vfill

\begin{center}
    \bfseries
    Москва, \the\year
\end{center}
\end{titlepage}

\pagebreak

\section*{Лабораторная работа \textnumero 1 \enquote{Добыча корпуса документов}} 

Необходимо подготовить корпус документов, который будет использован при выполнении остальных лабораторных работ:
\begin{itemize}
    \item Скачать его к себе на компьютер. В отчёте нужно указать источник данных.
    \item Ознакомиться с ним, изучить его характеристики. Из чего состоит текст? Есть ли
дополнительная мета-информация? Если разметка текста, какая она?
    \item Разбить на документы.
    \item Выделить текст.
    \item Найти существующие поисковики, которые уже можно использовать для поиска по
выбранному набору документов (встроенный поиск Википедии, поиск Google с использованием ограничений на URL или на сайт). Если такого поиска найти невозможно, то использовать корпус для выполнения лабораторных работ нельзя!
    \item Привести несколько примеров запросов к существующим поисковикам, указать недостатки в полученной поисковой выдаче.
\end{itemize}

В результатах работы должна быть указаны статистическая информация о корпусе:
\begin{itemize}
    \item Размер \enquote{сырых} данных.
    \item Количество документов.
    \item Размер текста, выделенного из \enquote{сырых} данных.
    \item Средний размер документа, средний объём текста в документе.
\end{itemize}

\pagebreak

\section*{Описание}

Требуется выбрать корпус документов, по которому в следующих лабораторных работах буду их использовать, ознакомиться с корпусами и проанализировать их HTML код, привести примеры поисковых запросов к выбранному корпусу документов.

\section*{Источник данных}
Для формирования корпуса были выбраны ресурсы, посвященные аниме-культуре и рецензиям:
\begin{itemize}
    \item \textbf{Shikimori} \url{https://shikimori.one} — крупнейшая русскоязычная энциклопедия аниме и манги.
    \item \textbf{Kanobu} \url{https://kanobu.ru} — популярный портал о видеоиграх и поп-культуре, содержащий развернутые рецензии.
\end{itemize}

\textbf{Обоснование выбора:}
Тексты рецензий на данных ресурсах написаны естественным языком, содержат специфическую терминологию, сленг и оценочные суждения. Это делает корпус репрезентативным для проверки алгоритмов стемминга и поиска (закон Ципфа на таких текстах проявляется особенно ярко).

\section*{Предварительный анализ структуры документов}
Каждый документ представляет собой HTML-страницу. В ходе анализа выявлены следующие особенности:
\begin{itemize}
	\item \textbf{Shikimori}: Текст рецензии находится в блоках \texttt{<div>} с классом \texttt{b-review-topic}. Заголовки часто отсутствуют (используется никнейм автора), поэтому в качестве заголовка берется название тайтла.
	\item \textbf{Kanobu}: Использует сложную верстку. Текст статьи расположен в блоках \texttt{c-entry\_\_body} или размечается атрибутом \texttt{itemprop="articleBody"}.
\end{itemize}

\section*{Примеры документов}
 Пример документа с \textbf{shikimori.one}: \\

\begin{itemize}
    \item \textit{Размер 'сырых' данных (HTML)}: 1630.89 KB
    \item \textit{Размер выделенного текста}: 548.01 KB
    \item \textit{Средний размер документа (HTML)}: 12945.96 bytes
    \item \textit{Средний объем текста текста}: 4350.07 bytes
\end{itemize}

\section*{Поисковые запросы и анализ выдачи}
Для анализа использовался Google и Яндекс с оператором \textit{site:}.\\
Запросы в Google в 3ех видах - запрос к сайду shikimori.one, запрос к сайту kanobu.ru и к ним обоим: \\ \\
\includegraphics[width=80mm,height=50mm]{g1.png} \includegraphics[width=80mm,height=50mm]{g2.png} \\
\includegraphics[width=80mm,height=50mm]{g3.png}

Запросы в Yandex в 3ех видах - запрос к сайду shikimori.one, запрос к сайту kanobu.ru и к ним обоим: \\ \\
\includegraphics[width=80mm,height=70mm]{y1.png} \includegraphics[width=80mm,height=70mm]{y2.png} \\
\includegraphics[width=80mm,height=70mm]{y3.png} \\ 

\section*{Вывод}

В ходе работы был сформирован корпус из 547 документов. Соотношение объема чистого текста к сырому HTML составляет примерно 30\%, что говорит об эффективной очистке данных от тегов и служебной информации. Собранный материал готов для использования в задачах индексации.

\begin{thebibliography}{99}
\bibitem{Kormen}
Маннинг, Рагхаван, Шютце
{\itshape Введение в информационный поиск} --- Издательский дом \enquote{Вильямс}, 2011. Перевод с английского: доктор физ.-мат. наук Д.\,А.\, Клюшина --- 528 с. (ISBN 978-5-8459-1623-4 (рус.))

\bibitem{shikimori} Shikimori.one: Шикимори - энциклопедия аниме и манги \\
\url {https://shikimori.one}

\bibitem{kanobu} Kanobu.ru: Канобу — медиа о поп-культуре, кино, сериалах и аниме \\  \url{https://kanobu.ru}.

\end{thebibliography}
\pagebreak

\section*{Лабораторная работа \textnumero 2 \enquote{Поисковый робот}} 

Необходимо написать парсер на любом языке программирования:
\begin{itemize}
	\item Написать поисковый робот — компоненты обкачки документов, используя любой язык программирования.
	\item Единственным аргументом поисковому роботу подаётся файл конфигурации (формата YAML или JSON), в котором содержатся параметры работы программы.
	\item База данных должна быть запущена в docker-контейнере, можно использовать docker-compose.
	\item В качестве хранилища результатов использовать MongoDB или PostgreSQL.
	\item Робот должен применять нормализацию URL-адресов.
	\item Робот должен сохранять в БД сырой HTML документа.
	\item Необходимо сохранять метаинформацию о каждом документе (дата скачивания, источник URL и т.д.).
	\item При остановке работы робот должен сохранять контрольную точку так, чтобы при повторном запуске он мог продолжить работу с того документа, с которого он остановился.
	\item Периодически он должен уметь переобкачивать документы, которые уже есть в базе, но только в том случае, если они изменились.
\end{itemize}

\pagebreak

\section*{Задание}
Разработать поисковый робот (crawler), который:
\begin{itemize}
	\item Обходит целевые веб-ресурсы (Shikimori, Kanobu).
	\item Сохраняет данные в базу данных MongoDB.
	\item Поддерживает конфигурацию через YAML-файл.
	\item Умеет останавливаться и возобновлять работу (checkpoint).
	\item Избегает повторного скачивания актуальных документов.
\end{itemize}
\section*{Выбор языка программирования и технологий}

Для реализации поискового робота был выбран язык программирования \textbf{Python} \\

\section*{Архитектура и технологии}
\textbf{Язык программирования:} Python 3.10 \\
\textbf{База данных:} MongoDB (использована библиотека \texttt{pymongo}). \\
\textbf{Библиотеки:} \texttt{requests} (HTTP-запросы), \texttt{BeautifulSoup4} (парсинг HTML), \texttt{PyYAML} (конфигурация).

\section*{Реализация}

\subsection*{1. Конфигурация (config.yaml)}
Параметры робота вынесены в отдельный файл. Это позволяет менять задержку (delay) и источники без правки кода.
\begin{lstlisting}
	db:
	host: "localhost"
	port: 27017
	name: "ir_lab"
	collection: "documents"
	logic:
	delay: 2.0
	reindex_days: 7
\end{lstlisting}

\subsection*{2. Логика обкачки и парсинга}
Робот использует адаптивные селекторы. Для сайта Kanobu реализован механизм поиска текста по нескольким вероятным классам (\texttt{c-entry\_\_body}, \texttt{itemprop="articleBody"}), что делает парсер устойчивым к изменениям верстки.

\subsection*{3. Сохранение состояния (Checkpoint)}
Для реализации остановки и возобновления робот сохраняет номер последней обработанной страницы в файл \texttt{crawler\_state.json}.
\begin{lstlisting}
	def save_state(state):
	with open('crawler_state.json', 'w') as f:
	json.dump(state, f)
\end{lstlisting}

\subsection*{4. Работа с базой данных}
Документы сохраняются в MongoDB. Перед записью проверяется поле \texttt{timestamp}. Если документ уже есть в базе и с момента последнего скачивания прошло менее 7 дней (настройка \texttt{reindex\_days}), повторное скачивание не производится.

\section*{Результаты работы}
Робот успешно обошел целевые сайты. В базу данных было сохранено 547 документов за примерно 25-30 минут работы программы.
\begin{itemize}
	\item \textbf{Стабильность:} Робот корректно обрабатывает ошибки 404 и временные разрывы соединения.
	\item \textbf{Производительность:} Среднее время обработки одной страницы (с учетом задержки вежливости) составило $\approx 2.5$ сек.
\end{itemize}

Пример структуры сохраненного документа (JSON):
\begin{verbatim}
	{
		"_id": ObjectId("..."),
		"url": "https://shikimori.one/animes/...",
		"source": "shikimori",
		"clean_text": "Потрясающий сюжет и...",
		"timestamp": 1703885000
	}
\end{verbatim}

\section*{Реализация механизмов отказоустойчивости}

В процессе написания поискового робота особое внимание было уделено стабильности процесса обкачки при работе с внешними ресурсами. Реализованы следующие механизмы:

\begin{itemize}
	\item \textbf{Обработка HTTP-ошибок:} Робот анализирует коды ответа сервера. При получении ошибок доступа (например, \texttt{403 Forbidden}) или сетевых таймаутов, программа не завершается аварийно, а логирует событие и переходит к следующему документу.
	\item \textbf{Адаптивный сбор данных:} При возникновении предупреждений об отсутствии контента (\texttt{WARN}), робот использует резервные эвристики (поиск по альтернативным селекторам или сбор всех параграфов текста), что позволяет минимизировать потери данных при изменении верстки сайта.
	\item \textbf{Безопасная остановка:} Перехвачен сигнал \texttt{KeyboardInterrupt} (\texttt{Ctrl+C}). При нажатии данной комбинации робот немедленно прекращает сетевую активность, финализирует текущую итерацию и сохраняет текущий индекс страницы каталога в файл состояния.
\end{itemize}

\section*{Управление состоянием и фильтрация дубликатов}

Для обеспечения эффективности и возможности работы на больших объемах данных реализованы:

\begin{enumerate}
	\item \textbf{Механизм контрольных точек (Checkpoint):} Текущий прогресс обкачки (номера страниц каталога для каждого источника) сохраняется в JSON-файл. При повторном запуске робот считывает состояние и продолжает работу именно с того места, где она была прервана.
	\item \textbf{Исключение дубликатов:} 
	\begin{itemize}
		\item На этапе сбора ссылок используется структура \texttt{set()} для удаления повторяющихся URL на одной странице.
		\item На этапе сохранения в MongoDB реализована логика \texttt{upsert} на основе уникального индекса по полю \texttt{url}. Если документ с таким адресом уже существует, робот проверяет дату его обновления. Если данные актуальны, обкачка «сырого» контента пропускается, что значительно экономит трафик и ресурсы БД.
	\end{itemize}
\end{enumerate}

\section*{Результаты тестирования механизмов управления}

В ходе выполнения работы были проведены тесты на прерывание процесса:
\begin{itemize}
	\item \textbf{Тест остановки:} Принудительная остановка через \texttt{Ctrl+C} на 5-й странице каталога. Лог подтвердил сохранение: \textit{«Робот остановлен пользователем. Прогресс сохранен»}.
	\item \textbf{Тест восстановления:} Повторный запуск инициировал запрос сразу к 5-й странице, пропустив первые четыре.
	\item \textbf{Тест дубликатов:} Повторная обкачка уже существующих 547 документов заняла значительно меньше времени за счет срабатывания логики \texttt{[SKIP] Актуально}.
\end{itemize}

\section*{Вывод}
Разработанный краулер удовлетворяет всем требованиям задания. Использование NoSQL базы данных MongoDB позволило эффективно хранить неструктурированные данные (HTML и текст) и метаинформацию.

\begin{thebibliography}{99}
	\bibitem{Manning}
	Маннинг, Рагхаван, Шютце
	{\itshape Введение в информационный поиск} --- Издательский дом \enquote{Вильямс}, 2011. --- 528 с.
	
	\bibitem{mongodb}
	MongoDB Go Driver Documentation \\ \url{https://www.mongodb.com/docs/drivers/go/current/}
	
	\bibitem{shikimori} Shikimori.one: Шикимори - энциклопедия аниме и манги \\
	\url {https://shikimori.one}

	\bibitem{kanobu} Kanobu.ru: Канобу — медиа о поп-культуре, кино, сериалах и аниме \\  \url{https://kanobu.ru}.
	
	\bibitem{webcrawler}
	Web Crawler Architecture and Design \\ \url{https://en.wikipedia.org/wiki/Web_crawler}
	
	\bibitem{robotstxt}
	The Robots Exclusion Protocol \\ \url{https://www.robotstxt.org/}
	
\end{thebibliography}
\pagebreak

\section*{Лабораторная работа \textnumero 3 \enquote{Токенизация, стемминг, закон Ципфа, индексация и булев поиск}}

Реализовать токенизацию собранного корпуса, построить инвертированный индекс, проверить закон Ципфа и реализовать булев поиск.

\subsection*{Часть 1. Токенизация}
\begin{itemize}
	\item Реализовать процесс разбиения текстов документов на токены.
	\item Выработать правила токенизации, описать их достоинства и недостатки.
	\item Привести примеры неудачно выделенных токенов и способы исправления.
	\item Указать статистику: количество токенов, среднюю длину, скорость обработки.
\end{itemize}

\subsection*{Часть 2. Закон Ципфа}
\begin{itemize}
	\item Построить график распределения терминов по частотности в логарифмической шкале.
	\item Наложить теоретический закон Ципфа на реальные данные.
	\item Объяснить причины расхождения.
	\item (Опционально) Подобрать константы для закона Мандельброта.
\end{itemize}

\subsection*{Часть 3. Лемматизация}
\begin{itemize}
	\item Добавить лемматизацию/стемминг в поисковую систему.
	\item Оценить качество поиска до и после внедрения.
	\item Проанализировать запросы, где качество ухудшилось, объяснить причины.
\end{itemize}

\subsection*{Часть 4. Булев поиск}
\begin{itemize}
	\item Реализовать инвертированный индекс.
	\item Реализовать булев поиск с операторами AND, OR, NOT.
	\item Провести тестирование на реальных запросах.
\end{itemize}

\pagebreak

\section*{1. Токенизация и Стемминг}
Процесс преобразования текста в поисковые термины состоял из этапов:
\begin{enumerate}
	\item \textbf{Очистка:} Удаление знаков препинания и спецсимволов (использовано регулярное выражение \texttt{[а-яёa-z]+}).
	\item \textbf{Нормализация:} Приведение к нижнему регистру.
	\item \textbf{Фильтрация:} Удаление стоп-слов (предлоги, союзы) с использованием библиотеки \texttt{NLTK}.
	\item \textbf{Стемминг:} Приведение слов к основе (стемме) с помощью алгоритма Snowball (RussianStemmer).
\end{enumerate}

\section*{2. Статистика обработки}
Анализ корпуса из 547 документов показал следующие результаты:

\begin{table}[H]
	\centering
	\caption{Результаты токенизации}
	\begin{tabular}{lc}
		\toprule
		\textbf{Метрика} & \textbf{Значение} \\
		\midrule
		Общее количество токенов & 103,925 \\
		Средняя длина токена & 6.99 символов \\
		Время выполнения индексации & 2.2542 сек \\
		Скорость обработки & 859.80 KB/sec \\
		\bottomrule
	\end{tabular}
\end{table}

Скорость обработки $\approx 860$ КБ/сек является приемлемой для скриптового языка Python, учитывая затраты на стемминг каждого слова.

\section*{3. Закон Ципфа}

\subsection*{Теория}

\textbf{Закон Ципфа} — эмпирическое наблюдение, согласно которому частота слова в тексте обратно пропорциональна его рангу в частотном словаре. Математически выражается как:

\[
f(r) = \frac{C}{r}
\]

где:
\begin{itemize}
	\item \( f(r) \) — частота термина с рангом \( r \)
	\item \( r \) — ранг термина (1 для самого частого, 2 для второго и т.д.)
	\item \( C \) — константа, равная частоте самого частого слова
\end{itemize}

\textbf{Закон Мандельброта} — обобщение закона Ципфа с дополнительными параметрами:

\[
f(r) = \frac{C}{(r + B)^\alpha}
\]

где:
\begin{itemize}
	\item \( B \) — сдвиг, учитывающий отклонение высокочастотных слов
	\item \( \alpha \) — показатель степени (для классического Ципфа \(\alpha = 1\))
\end{itemize}

Был построен график распределения частот терминов в логарифмической шкале (Log-Log).

\begin{figure}[H]
	\centering
	\includegraphics[width=80mm,height=70mm]{zipf_lab3.png}
	\caption{График распределения частот терминов (Закон Ципфа)}
\end{figure}

\textbf{Анализ графика:} Полученная кривая близка к линейной, что подтверждает выполнение закона Ципфа для собранного корпуса. Наиболее частотные слова имеют ранг 1, а частота убывает обратно пропорционально рангу.

\section*{4. Реализация поиска}
Реализован \textbf{инвертированный индекс} — структура данных, сопоставляющая каждому термину список ID документов. На его основе построен булев поиск с поддержкой операторов \texttt{AND}, \texttt{OR}, \texttt{NOT}.

\subsection*{Пример работы поиска}
\textbf{Запрос:} \texttt{сюжет AND персонажи} \\
\textbf{Логика:} Поиск документов, где встречаются основы слов «сюжет» и «персонаж» одновременно.

\textbf{Результаты:}
\begin{itemize}
	\item Найдено документов: 182
	\item Примеры выдачи:
	\begin{enumerate}
		\item \url{https://shikimori.one/animes/56752-shiguang-dailiren-yingdu-pian/reviews#580253}
		\item \url{https://shikimori.one/animes/47194-summertime-render/reviews#611042}
		\item \url{https://shikimori.one/animes/z37430-tensei-shitara-slime-datta-ken/reviews#614750}
	\end{enumerate}
\end{itemize}

\section*{Вывод}
Реализованная поисковая система неплохо выполняет индексацию и поиск по корпусу из 547 документов. Использование стемминга позволяет находить документы, содержащие различные словоформы (например, «персонажи», «персонажей»), что повышает полноту поиска (Recall). Время отклика поиска по инвертированному индексу составляет доли секунды.

\begin{thebibliography}{99}
	\bibitem{Manning}
	Маннинг, Рагхаван, Шютце
	{\itshape Введение в информационный поиск} --- Издательский дом \enquote{Вильямс}, 2011. --- 528 с.
	
	\bibitem{Zipf}
	Zipf, G. K. (1949). {\itshape Human Behavior and the Principle of Least Effort}. Addison-Wesley.
	
	\bibitem{Mandelbrot}
	Mandelbrot, B. (1953). {\itshape An Informational Theory of the Statistical Structure of Language}. Communication Theory.
	
	\bibitem{Porter}
	Porter, M. F. (1980). {\itshape An algorithm for suffix stripping}. Program, 14(3), 130–137.
	
	\bibitem{shikimori} Shikimori.one: Шикимори - энциклопедия аниме и манги \\
	\url {https://shikimori.one}
	
	\bibitem{kanobu} Kanobu.ru: Канобу — медиа о поп-культуре, кино, сериалах и аниме \\  
	\url{https://kanobu.ru}.
\end{thebibliography}

\end{document}
